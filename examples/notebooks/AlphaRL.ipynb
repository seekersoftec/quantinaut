{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMbBrCZTwDQvTcdEFEkT/Nh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JoI-lSCiKAIx"},"source":["# Install Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YysbEssgKAIy"},"outputs":[],"source":["!pip install ccxt -qqq\n","!pip install yfinance -qqq\n","!pip install mplfinance -qqq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bxzLAo2u4Kx"},"outputs":[],"source":["!pip install tensorflow -qqq\n","!pip install keras -qqq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KE579cqTC_vL"},"outputs":[],"source":["!pip install gymnasium -qqq\n","!pip install stable-baselines3 -qqq\n","!pip install stable-baselines3[extra] -qqq\n","!pip install sb3_contrib -qqq"]},{"cell_type":"markdown","metadata":{"id":"Y4Mfp_elC-pv"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCtjsYmZKAI0"},"outputs":[],"source":["import sys\n","import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import yfinance as yf\n","import statsmodels.api as sm\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","from matplotlib.animation import FuncAnimation\n","from numpy.linalg import norm\n","from scipy.stats import entropy\n","from scipy.stats import entropy\n","from sklearn.cluster import Birch\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","import ccxt\n","import logging\n","from pathlib import Path\n","from typing import List, Optional, Union\n","\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"ENWUYWjxKLYe"},"source":["# Load Data\n","\n","\n","From any ccxt supported exchange.\n","\n","coinbase, gemini, kraken, binanceus, etc\n","\n","Recommended => Coinbase.\n","\n"]},{"cell_type":"markdown","source":["## CCXT"],"metadata":{"id":"_8gDg1IJJ2ZF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqquFOtTtwwS"},"outputs":[],"source":["INSTRUMENT = \"SOL/USDT\"\n","TIMEFRAME = \"4h\"\n","EXCHANGE_ID = \"binanceus\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1349,"status":"ok","timestamp":1750353556807,"user":{"displayName":"Quants Pub","userId":"02620188949501282047"},"user_tz":-60},"id":"S9ey8z_Uteln","outputId":"a0905ac9-b5b6-4509-e786-345b4d610099"},"outputs":[{"output_type":"stream","name":"stdout","text":["Supported timeframes for Binanceus:\n","1s\n","1m\n","3m\n","5m\n","15m\n","30m\n","1h\n","2h\n","4h\n","6h\n","8h\n","12h\n","1d\n","3d\n","1w\n","1M\n"]}],"source":["# Instantiate the Coinbase exchange\n","exchange: ccxt.Exchange = getattr(ccxt, EXCHANGE_ID)()\n","\n","# Load the markets to get exchange information, including timeframes\n","exchange.load_markets()\n","\n","# Get and print the supported timeframes\n","exchange_timeframes = exchange.timeframes\n","\n","if exchange_timeframes:\n","    print(f\"Supported timeframes for {EXCHANGE_ID.capitalize()}:\")\n","    for timeframe in exchange_timeframes:\n","        print(timeframe)\n","else:\n","    print(f\"Could not retrieve timeframes for {EXCHANGE_ID.capitalize()}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-wDWsMfKLYf"},"outputs":[],"source":["# Configure logger\n","logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","logger = logging.getLogger(__name__)\n","\n","# Add a log message to see output\n","logger.info(\"Logger configured successfully.\")\n","\n","def fetch_ohlcv_with_retries(exchange: ccxt.Exchange, symbol: str, timeframe: str, since: int, limit: int, max_retries: int = 3) -> List[List[Union[int, float]]]:\n","    \"\"\"Fetch OHLCV data with retry logic.\"\"\"\n","    for attempt in range(max_retries):\n","        try:\n","            return exchange.fetch_ohlcv(symbol, timeframe, since, limit)\n","        except Exception as e:\n","            if attempt == max_retries - 1:\n","                logger.error(f\"Failed to fetch {timeframe} {symbol} OHLCV after {max_retries} attempts: {e}\")\n","                raise\n","    return []\n","\n","def load_existing_data(filename: Path) -> pd.DataFrame:\n","    \"\"\"Load existing OHLCV data if available.\"\"\"\n","    if filename.exists():\n","        return pd.read_csv(filename, parse_dates=[\"timestamp\"], index_col=\"timestamp\")\n","    return pd.DataFrame(columns=[\"open\", \"high\", \"low\", \"close\", \"volume\"])\n","\n","def scrape_ohlcv(exchange: ccxt.Exchange, symbol: str, timeframe: str, since: int, until: int, limit: int, max_retries: int = 3) -> List[List[Union[int, float]]]:\n","    \"\"\"Scrape historical OHLCV data from an exchange between two dates.\"\"\"\n","    all_ohlcv: List[List[Union[int, float]]] = []\n","\n","    while since < until:\n","        ohlcv: List[List[Union[int, float]]] = fetch_ohlcv_with_retries(exchange, symbol, timeframe, since, limit, max_retries)\n","\n","        if not ohlcv:\n","            break\n","\n","        since = ohlcv[-1][0] + 1  # Move forward in time\n","        all_ohlcv.extend(ohlcv)\n","        logger.info(f\"{len(all_ohlcv)} {symbol} candles collected from {exchange.iso8601(all_ohlcv[0][0])} to {exchange.iso8601(all_ohlcv[-1][0])}\")\n","\n","    return all_ohlcv\n","\n","def save_to_csv(filename: Path, data: pd.DataFrame) -> None:\n","    \"\"\"Save OHLCV data to a CSV file, appending new data if necessary.\"\"\"\n","    if filename.exists():\n","        data.to_csv(filename, mode='a', header=False)\n","    else:\n","        data.to_csv(filename)\n","    logger.info(f\"Data saved to {filename}\")\n","\n","def scrape_and_save_candles(exchange_id: str, symbol: str, timeframe: str, since: Union[int, str], until: Union[int, str], limit: int, max_retries: int = 3, filename: Optional[str] = None, exchange_options: Optional[dict] = None) -> None:\n","    \"\"\"Scrape OHLCV data and save to a CSV file, supporting resuming downloads.\"\"\"\n","\n","    if filename is None or len(filename) == 0:\n","        filename = f\"{symbol.replace('/', '_')}_{timeframe}.csv\".lower()\n","\n","    exchange_options = exchange_options or {}\n","    exchange: ccxt.Exchange = getattr(ccxt, exchange_id)({'enableRateLimit': True, 'options': exchange_options})\n","\n","    if isinstance(since, str):\n","        since = exchange.parse8601(since)\n","    if not until:\n","        until = exchange.milliseconds()\n","    elif isinstance(until, str):\n","        until = exchange.parse8601(until)\n","\n","    exchange.load_markets()\n","    file_path = Path(\"./data/ccxt/\") / exchange_id / filename\n","    file_path.parent.mkdir(parents=True, exist_ok=True)\n","    existing_data = load_existing_data(file_path)\n","\n","    if not existing_data.empty:\n","        last_timestamp = existing_data.index[-1].timestamp() * 1000  # Convert to ms\n","        if last_timestamp > since:\n","            since = int(last_timestamp) + 1  # Resume from the next candle\n","\n","    ohlcv = scrape_ohlcv(exchange, symbol, timeframe, since, until, limit, max_retries)\n","\n","    if ohlcv:\n","        new_data = pd.DataFrame(ohlcv, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n","        new_data[\"timestamp\"] = pd.to_datetime(new_data[\"timestamp\"], unit='ms')\n","        if not new_data.empty:\n","            if not existing_data.empty:\n","                existing_data.reset_index(inplace=True)\n","                combined_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n","            else:\n","                combined_data = new_data.drop_duplicates(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n","            save_to_csv(file_path, combined_data)\n","            logger.info(f\"Saved {len(new_data)} new candles from {new_data.iloc[0, 0]} to {new_data.iloc[-1, 0]} to {filename}\")\n","        else:\n","            logger.warning(\"No new OHLCV data to save.\")\n","    else:\n","        logger.warning(\"No new OHLCV data retrieved.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322,"status":"ok","timestamp":1750353562309,"user":{"displayName":"Quants Pub","userId":"02620188949501282047"},"user_tz":-60},"id":"E1dialnqY10k","outputId":"46e72934-b271-4610-f24d-f318a0795243"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('binanceus', 'SOL/USDT', '4h')"]},"metadata":{},"execution_count":15}],"source":["EXCHANGE_ID, INSTRUMENT, TIMEFRAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPBD5HjHtEXx"},"outputs":[],"source":["scrape_and_save_candles(exchange_id=EXCHANGE_ID, symbol=INSTRUMENT, timeframe=TIMEFRAME,\n","                            since=\"2019-06-01T00:00:00Z\", until=\"2025-06-19T23:59:59Z\", limit=1000)\n","\n","# Exchange options:\n","# scrape_and_save_candles(\"binance\", \"BTC/USDT\", \"4h\", \"2011-01-01T00:00:00Z\", \"2023-12-01T00:00:00Z\", 1000, exchange_options={'defaultType': 'future'})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1750353583961,"user":{"displayName":"Quants Pub","userId":"02620188949501282047"},"user_tz":-60},"id":"UNOFWV1YKLYg","outputId":"9db0e747-49ba-40dd-ee40-1c70ad227105"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                timestamp      open      high       low     close    volume\n","0     2020-09-18 12:00:00    3.0887    3.1355    2.8178    2.8929  5938.630\n","1     2020-09-18 16:00:00    2.9105    3.1543    2.8191    3.1487  9460.390\n","2     2020-09-18 20:00:00    3.1429    3.1490    3.0340    3.0994  1170.880\n","3     2020-09-19 00:00:00    3.0960    3.2430    3.0946    3.1240  3186.390\n","4     2020-09-19 04:00:00    3.1298    3.1453    3.0708    3.1044   327.100\n","...                   ...       ...       ...       ...       ...       ...\n","10405 2025-06-19 00:00:00  146.4400  147.7400  145.0000  146.4300   100.995\n","10406 2025-06-19 04:00:00  146.6700  147.2600  145.1400  145.2900    75.422\n","10407 2025-06-19 08:00:00  145.7800  146.4800  144.9200  145.1800   251.374\n","10408 2025-06-19 12:00:00  145.2500  145.4300  143.1800  143.5000   435.471\n","10409 2025-06-19 16:00:00  143.6700  144.1400  143.4500  143.8700    15.503\n","\n","[10410 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-b049e703-e80e-4647-a11d-4ec057888e33\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>timestamp</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-09-18 12:00:00</td>\n","      <td>3.0887</td>\n","      <td>3.1355</td>\n","      <td>2.8178</td>\n","      <td>2.8929</td>\n","      <td>5938.630</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-09-18 16:00:00</td>\n","      <td>2.9105</td>\n","      <td>3.1543</td>\n","      <td>2.8191</td>\n","      <td>3.1487</td>\n","      <td>9460.390</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-09-18 20:00:00</td>\n","      <td>3.1429</td>\n","      <td>3.1490</td>\n","      <td>3.0340</td>\n","      <td>3.0994</td>\n","      <td>1170.880</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-09-19 00:00:00</td>\n","      <td>3.0960</td>\n","      <td>3.2430</td>\n","      <td>3.0946</td>\n","      <td>3.1240</td>\n","      <td>3186.390</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-09-19 04:00:00</td>\n","      <td>3.1298</td>\n","      <td>3.1453</td>\n","      <td>3.0708</td>\n","      <td>3.1044</td>\n","      <td>327.100</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10405</th>\n","      <td>2025-06-19 00:00:00</td>\n","      <td>146.4400</td>\n","      <td>147.7400</td>\n","      <td>145.0000</td>\n","      <td>146.4300</td>\n","      <td>100.995</td>\n","    </tr>\n","    <tr>\n","      <th>10406</th>\n","      <td>2025-06-19 04:00:00</td>\n","      <td>146.6700</td>\n","      <td>147.2600</td>\n","      <td>145.1400</td>\n","      <td>145.2900</td>\n","      <td>75.422</td>\n","    </tr>\n","    <tr>\n","      <th>10407</th>\n","      <td>2025-06-19 08:00:00</td>\n","      <td>145.7800</td>\n","      <td>146.4800</td>\n","      <td>144.9200</td>\n","      <td>145.1800</td>\n","      <td>251.374</td>\n","    </tr>\n","    <tr>\n","      <th>10408</th>\n","      <td>2025-06-19 12:00:00</td>\n","      <td>145.2500</td>\n","      <td>145.4300</td>\n","      <td>143.1800</td>\n","      <td>143.5000</td>\n","      <td>435.471</td>\n","    </tr>\n","    <tr>\n","      <th>10409</th>\n","      <td>2025-06-19 16:00:00</td>\n","      <td>143.6700</td>\n","      <td>144.1400</td>\n","      <td>143.4500</td>\n","      <td>143.8700</td>\n","      <td>15.503</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10410 rows Ã— 6 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b049e703-e80e-4647-a11d-4ec057888e33')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b049e703-e80e-4647-a11d-4ec057888e33 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b049e703-e80e-4647-a11d-4ec057888e33');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-50134869-0550-4886-ad49-2a11715932a4\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50134869-0550-4886-ad49-2a11715932a4')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-50134869-0550-4886-ad49-2a11715932a4 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_d03375eb-a8cb-406e-990d-398167daf12f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ohlcv')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_d03375eb-a8cb-406e-990d-398167daf12f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('ohlcv');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"ohlcv","summary":"{\n  \"name\": \"ohlcv\",\n  \"rows\": 10410,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-09-18 12:00:00\",\n        \"max\": \"2025-06-19 16:00:00\",\n        \"num_unique_values\": 10410,\n        \"samples\": [\n          \"2022-07-09 08:00:00\",\n          \"2022-10-24 20:00:00\",\n          \"2025-02-08 20:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70.54851716006726,\n        \"min\": 1.1772,\n        \"max\": 289.16,\n        \"num_unique_values\": 9219,\n        \"samples\": [\n          213.83,\n          43.7891,\n          1.9242\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"high\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 71.55078968291798,\n        \"min\": 1.2846,\n        \"max\": 294.94,\n        \"num_unique_values\": 9020,\n        \"samples\": [\n          42.6094,\n          61.39,\n          41.0024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 69.48722209227493,\n        \"min\": 1.0818,\n        \"max\": 266.76,\n        \"num_unique_values\": 8970,\n        \"samples\": [\n          238.38,\n          113.183,\n          150.84\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70.54905573547154,\n        \"min\": 1.2012,\n        \"max\": 288.87,\n        \"num_unique_values\": 9217,\n        \"samples\": [\n          30.3165,\n          106.9495,\n          182.29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20150.409752437874,\n        \"min\": 0.0,\n        \"max\": 532238.68,\n        \"num_unique_values\": 10362,\n        \"samples\": [\n          12533.01,\n          405.354,\n          4231.22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":17}],"source":["ohlcv = pd.read_csv(f\"./data/ccxt/{EXCHANGE_ID}/{INSTRUMENT.replace('/', '_')}_{TIMEFRAME}.csv\".lower(), parse_dates=[\"timestamp\"])\n","if \"Unnamed: 0\" in ohlcv.columns:\n","    ohlcv.drop(columns=[\"Unnamed: 0\"], inplace=True)\n","\n","ohlcv\n"]},{"cell_type":"markdown","metadata":{"id":"WagEbMBLJKq5"},"source":["## Yahoo Finance\n","\n","\n","No enough data for the following:\n","- 'PI35697-USD'\n","\n","\n","**Indices for Alpha Generation:**\n","- USD Dollar Index (DXY/USDX/DX-Y.NYB)\n","- Trade-Weighted Dollar Index (DTWEXBGS)\n","\n","*Why they matter for alpha generation:*\n","- They are proxies of **USD strength**, impacting global liquidity, capital flows, and inflation expectations.\n","- many risk assets, commodities, emerging markets, and cryptocurrencies are inversely correlated withthe dollar.\n","- Useful for both directional strategies and regime-based filters.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15431,"status":"ok","timestamp":1750353630453,"user":{"displayName":"Quants Pub","userId":"02620188949501282047"},"user_tz":-60},"id":"-1xQ7HKPqK3C","outputId":"e494599d-bee8-48a2-e024-b7d9955bf6f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fetching data for ^GSPC...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n","[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for ^GSPC to ./data/yahoo/^GSPC.csv\n","Fetching data for ^DJI...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for ^DJI to ./data/yahoo/^DJI.csv\n","Fetching data for ^IXIC...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for ^IXIC to ./data/yahoo/^IXIC.csv\n","Fetching data for ^RUT...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for ^RUT to ./data/yahoo/^RUT.csv\n","Fetching data for ^VIX...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for ^VIX to ./data/yahoo/^VIX.csv\n","Fetching data for DX-Y.NYB...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for DX-Y.NYB to ./data/yahoo/DX_Y.NYB.csv\n","Fetching data for AAPL...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for AAPL to ./data/yahoo/AAPL.csv\n","Fetching data for AMZN...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for AMZN to ./data/yahoo/AMZN.csv\n","Fetching data for MSFT...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for MSFT to ./data/yahoo/MSFT.csv\n","Fetching data for NVDA...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for NVDA to ./data/yahoo/NVDA.csv\n","Fetching data for TSLA...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for TSLA to ./data/yahoo/TSLA.csv\n","Fetching data for INTC...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for INTC to ./data/yahoo/INTC.csv\n","Fetching data for QUBT...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for QUBT to ./data/yahoo/QUBT.csv\n","Fetching data for COIN...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for COIN to ./data/yahoo/COIN.csv\n","Fetching data for META...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for META to ./data/yahoo/META.csv\n","Fetching data for EURUSD=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for EURUSD=X to ./data/yahoo/EURUSDX.csv\n","Fetching data for GBPUSD=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for GBPUSD=X to ./data/yahoo/GBPUSDX.csv\n","Fetching data for USDJPY=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for USDJPY=X to ./data/yahoo/USDJPYX.csv\n","Fetching data for USDCHF=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for USDCHF=X to ./data/yahoo/USDCHFX.csv\n","Fetching data for EURGBP=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for EURGBP=X to ./data/yahoo/EURGBPX.csv\n","Fetching data for EURNZD=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for EURNZD=X to ./data/yahoo/EURNZDX.csv\n","Fetching data for EURCHF=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for EURCHF=X to ./data/yahoo/EURCHFX.csv\n","Fetching data for GBPNZD=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for GBPNZD=X to ./data/yahoo/GBPNZDX.csv\n","Fetching data for GBPCHF=X...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for GBPCHF=X to ./data/yahoo/GBPCHFX.csv\n","Fetching data for ^XAU...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for ^XAU to ./data/yahoo/^XAU.csv\n","Fetching data for ^XAG...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","ERROR:yfinance:\n","1 Failed download:\n","ERROR:yfinance:['^XAG']: YFPricesMissingError('possibly delisted; no price data found  (1d 2019-06-01 -> 2025-06-18)')\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["No data found for ^XAG\n","Fetching data for CL=F...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for CL=F to ./data/yahoo/CLF.csv\n","Fetching data for BTC-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for BTC-USD to ./data/yahoo/BTC_USD.csv\n","Fetching data for ETH-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for ETH-USD to ./data/yahoo/ETH_USD.csv\n","Fetching data for SOL-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for SOL-USD to ./data/yahoo/SOL_USD.csv\n","Fetching data for XRP-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for XRP-USD to ./data/yahoo/XRP_USD.csv\n","Fetching data for LTC-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for LTC-USD to ./data/yahoo/LTC_USD.csv\n","Fetching data for AAVE-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for AAVE-USD to ./data/yahoo/AAVE_USD.csv\n","Fetching data for BNB-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for BNB-USD to ./data/yahoo/BNB_USD.csv\n","Fetching data for TRX-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for TRX-USD to ./data/yahoo/TRX_USD.csv\n","Fetching data for DOT-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed\n","/tmp/ipython-input-18-4131418319.py:36: FutureWarning: YF.download() has changed argument auto_adjust default to True\n","  df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n"]},{"output_type":"stream","name":"stdout","text":["Saved data for DOT-USD to ./data/yahoo/DOT_USD.csv\n","Fetching data for XLM-USD...\n"]},{"output_type":"stream","name":"stderr","text":["\r[*********************100%***********************]  1 of 1 completed"]},{"output_type":"stream","name":"stdout","text":["Saved data for XLM-USD to ./data/yahoo/XLM_USD.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# # Fetch AAPL data\n","# advpp_data = yf.download('AAPL', start='2020-01-01', end='2024-01-01')\n","\n","# # Display the first few rows of the dataframe\n","# advpp_data.head()\n","\n","# --- Step 1: Fetch BTC/USD 15-min data for the last 7 days ---\n","# ticker=\"BTC-USD\"\n","# interval=\"15m\"\n","# ticker_data = yf.download(ticker, interval=interval, period=\"7d\")\n","# ticker_data.dropna(inplace=True)\n","# ticker_data.columns = [col.lower() for col in ticker_data.columns]\n","# ticker_data\n","\n","# ---- Parameters ----\n","indices = ['^GSPC', '^DJI', '^IXIC', '^RUT', '^VIX', 'DX-Y.NYB']\n","stocks = ['AAPL', 'AMZN', 'MSFT', 'NVDA', 'TSLA', 'INTC', 'QUBT', 'COIN', 'META']\n","fx = ['EURUSD=X', 'GBPUSD=X', 'USDJPY=X', 'USDCHF=X', 'EURGBP=X', 'EURNZD=X', 'EURCHF=X', 'GBPNZD=X', 'GBPCHF=X']\n","metals = ['^XAU', '^XAG']\n","commodities = ['CL=F']\n","crypto = ['BTC-USD', 'ETH-USD', 'SOL-USD', 'XRP-USD', 'LTC-USD', 'AAVE-USD', 'BNB-USD', 'TRX-USD', 'DOT-USD', 'XLM-USD']\n","symbols = indices + stocks + fx + metals + commodities + crypto\n","start_date = '2018-06-01'\n","end_date = '2025-06-19'\n","interval = '1d'\n","\n","# Create a directory to store data if it doesn't exist\n","data_dir = './data/yahoo'\n","if not os.path.exists(data_dir):\n","    os.makedirs(data_dir)\n","\n","# Fetch and save data for each symbol individually\n","for symbol in symbols:\n","    try:\n","        print(f\"Fetching data for {symbol}...\")\n","        df = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n","        if not df.empty:\n","            df.columns = df.columns.droplevel(1)\n","            filename = os.path.join(data_dir, f'{symbol.replace(\"-\", \"_\").replace(\"=\", \"\")}.csv')\n","            df.to_csv(filename)\n","            print(f\"Saved data for {symbol} to {filename}\")\n","        else:\n","            print(f\"No data found for {symbol}\")\n","    except Exception as e:\n","        print(f\"Error fetching data for {symbol}: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ypkzBFTxeVY"},"outputs":[],"source":["\n","def get_yahoo_data(symbol):\n","    \"\"\"\n","    Loads historical data for a given symbol from a CSV file,\n","    cleans and validates it.\n","    \"\"\"\n","    # Replace characters that might cause issues in filenames\n","    filename_symbol = symbol.replace(\"-\", \"_\").replace(\"=\", \"\")\n","    filepath = os.path.join(data_dir, f'{filename_symbol}.csv')\n","\n","    try:\n","        # Load the data from the individual CSV file\n","        df = pd.read_csv(filepath, parse_dates=['Date'], index_col='Date')\n","        print(f\"\\nLoaded data for {symbol} from {filepath}\")\n","        print(df.head()) # Print the head of the loaded DataFrame (df)\n","    except FileNotFoundError:\n","        print(f\"Error: Data file for {symbol} not found at {filepath}.\")\n","        return pd.DataFrame() # Return empty DataFrame if file not found\n","    except Exception as e:\n","        print(f\"Error loading or parsing data for {symbol} from {filepath}: {e}\")\n","        return pd.DataFrame() # Return empty DataFrame on other errors\n","\n","    # Convert relevant columns to numeric, coercing errors\n","    # Apply to_numeric to the entire DataFrame or selected columns after renaming\n","    # Note: yfinance typically provides these as floats, but this is a good safeguard\n","    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n","        if col in df.columns:\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","        else:\n","            print(f\"Warning: Column '{col}' not found in data for {symbol}.\")\n","\n","\n","    # Optional: Drop rows where all critical values are NaN\n","    # Ensure the columns exist before trying to drop based on them\n","    critical_cols = [col for col in ['Close', 'High', 'Low'] if col in df.columns]\n","    if critical_cols:\n","        df.dropna(subset=critical_cols, how='all', inplace=True)\n","    else:\n","         # If critical columns are missing, the dataframe is likely not useful\n","         print(f\"Warning: Critical columns (Close, High, Low) missing for {symbol}. Returning empty DataFrame.\")\n","         return pd.DataFrame()\n","\n","\n","    # Require a minimum number of data points after cleaning\n","    if df.empty or len(df) < 10:\n","        print(f\"Warning: Not enough valid data points (less than 10) for symbol {symbol} after cleaning. Found {len(df)} rows.\")\n","        return pd.DataFrame()\n","\n","    return df\n"]},{"cell_type":"markdown","metadata":{"id":"WrrpplKc8M3J"},"source":["## Extract Price Series\n","\n","\n","## âœ… **Proposal**\n","\n","We want to construct a custom price like:\n","\n","$$\n","\\text{Price}_{\\text{custom}} = w_1 \\cdot \\text{High}_t + w_2 \\cdot \\text{Low}_t + w_3 \\cdot \\text{Close}_t\n","$$\n","\n","Where:\n","\n","* $w_1, w_2, w_3$ are weights, possibly adaptive or fixed.\n","* This synthetic series is then used as the \"price\" input into your features, indicators, or models.\n","\n","---\n","\n","## ðŸ” Why This Can Be Powerful\n","\n","Most indicators use **close-only**, which discards valuable intraday range information. A weighted blend:\n","\n","* Captures **intrabar structure**.\n","* Smooths volatility.\n","* Can encode **market sentiment** better than raw closes.\n","\n","---\n","\n","## ðŸ“ Common Weighted Price Schemes\n","\n","Here are some **existing techniques** that can inspire or be combined with your idea:\n","\n","### 1. **Typical Price**\n","\n","$$\n","\\text{TP} = \\frac{High + Low + Close}{3}\n","$$\n","\n","### 2. **Weighted Close**\n","\n","$$\n","\\text{WC} = \\frac{High + Low + 2 \\cdot Close}{4}\n","$$\n","\n","### 3. **OHLC Average**\n","\n","$$\n","\\text{OHLC} = \\frac{Open + High + Low + Close}{4}\n","$$\n","\n","### 4. **Mid Price**\n","\n","$$\n","\\text{Mid} = \\frac{High + Low}{2}\n","$$\n","\n","### 5. **Custom Weights (Your Idea)**\n","\n","$$\n","\\text{P}_{\\text{custom}} = \\alpha \\cdot H + \\beta \\cdot L + \\gamma \\cdot C \\quad \\text{where} \\quad \\alpha + \\beta + \\gamma = 1\n","$$\n","\n","You can learn these weights in a model, optimize for Sharpe, or define them heuristically.\n","\n","---\n","\n","## ðŸ’¡ How to Choose the Weights\n","\n","### 1. **Heuristic**\n","\n","Try:\n","\n","* $\\alpha = 0.25, \\beta = 0.25, \\gamma = 0.5$\n","* $\\gamma = 1.0$ (use close-only as baseline)\n","* Use volatility to scale H/L contributions.\n","\n","### 2. **Machine Learning / Optimization**\n","\n","* Use **Sharpe-ratio maximization** to learn optimal weights.\n","* Or use a **regression model** to predict next return and fit the weights that best predict it.\n","\n","### 3. **Reinforcement Learning Integration**\n","\n","* Let your RL agent learn the weights $\\alpha, \\beta, \\gamma$ as parameters over time.\n","* Plug into observation: `obs = price_custom`, where `price_custom = weighted(H, L, C)`.\n","\n","---\n","\n","## ðŸ§ª Implementation Snippet (Python)\n","\n","```python\n","def custom_price(high, low, close, alpha=0.3, beta=0.3, gamma=0.4):\n","    return alpha * high + beta * low + gamma * close\n","```\n","\n","Or, to optimize weights dynamically:\n","\n","```python\n","def adaptive_price(high, low, close, vol):\n","    alpha = 0.3 + 0.2 * vol\n","    beta = 0.3 - 0.1 * vol\n","    gamma = 1 - alpha - beta\n","    return alpha * high + beta * low + gamma * close\n","```\n","\n","---\n","\n","## ðŸ§  Strategic Use in a Trading Pipeline\n","\n","* Use as the **main input price** to:\n","\n","  * Indicators (RSI, MACD, Bollinger)\n","  * Pattern detectors (reversals, breakouts)\n","  * RL environments (in `price_matrix`)\n","* Apply it to **volume-weighted** or **volatility-adjusted** views\n","* Use difference or change in this price to detect **price aggression** or **liquidity pressure**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YdYsBIoKLYh"},"outputs":[],"source":["# === STEP 1: Get price series ===\n","# dates = pd.date_range(start=\"2021-01-01\", periods=15, freq=\"M\")\n","# prices = np.array([\n","#     30000, 33000, 29000, 35000, 34000,\n","#     38000, 36000, 42000, 40000, 48000,\n","#     47000, 52000, 50000, 58000, 62000\n","# ], dtype=float)\n","\n","# Extract price series\n","dates = ohlcv[\"timestamp\"].values\n","highs = ohlcv[\"high\"].values\n","lows = ohlcv[\"low\"].values\n","closes = ohlcv[\"close\"].values\n","volumes = ohlcv[\"volume\"].values\n","\n","prices = closes\n"]},{"cell_type":"markdown","metadata":{"id":"1CoSnTRm9Ic-"},"source":["# Advanced Decision Making\n","\n","\n","using Reinforcement Learning\n","\n","Use a conbinancetion of the Pivots and the Price prediciton\n","\n","to train an LSTM Policy, use a gymnasium env, use stable base lines 3, use the reccurentppo from sb3 contrib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NjTH_ZX-AX_W"},"outputs":[],"source":["import datetime\n","import gymnasium as gym\n","from enum import IntEnum\n","from stable_baselines3 import PPO\n","from sb3_contrib import RecurrentPPO\n","from stable_baselines3.common.env_util import make_vec_env"]},{"cell_type":"markdown","metadata":{"id":"okxOG_RMC0KR"},"source":["## Define Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77N6RzuSC0uh"},"outputs":[],"source":["class Action(IntEnum):\n","  LONG_ENTER = 0\n","  LONG_EXIT = 1\n","  SHORT_ENTER = 2\n","  SHORT_EXIT = 3\n","  NEUTRAL = 4"]},{"cell_type":"markdown","metadata":{"id":"3uTxXLirAimw"},"source":["## Define Environments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6A-RXcuJAg-G"},"outputs":[],"source":["# Define a custom Gymnasium environment\n","class TradingEnv(gym.Env):\n","    def __init__(self, data, window_size=60):\n","        super(TradingEnv, self).__init__()\n","\n","        self.data = data\n","        self.window_size = window_size\n","        self.current_step = self.window_size # Start after the initial window\n","        self.max_steps = len(self.data) - 1\n","\n","        # Action space: 0: Sell, 1: Hold, 2: Buy\n","        self.action_space = gym.spaces.Discrete(3)\n","\n","        # Observation space: We'll include the price window, the price prediction,\n","        # and the current price's relation to recent pivot points.\n","        # The size will depend on the window size + prediction features + pivot features.\n","        # Let's assume we add 1 for the price prediction and 2 for pivot relation (e.g., distance to nearest support/resistance).\n","        # This is a simplified example, you'll need to carefully design your observation space.\n","        self.observation_space = gym.spaces.Box(\n","            low=0, high=1, shape=(self.window_size + 1 + 2,), dtype=np.float32\n","        )\n","\n","        # Initial state\n","        self.reset()\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.current_step = self.window_size\n","        self.balance = 10000  # Starting balance\n","        self.shares_held = 0\n","        self.net_worth = self.balance\n","        self.positions = [] # Track buy/sell actions and prices\n","        self.episode_return = 0 # Track return for the episode\n","\n","        # Get the initial observation\n","        observation = self._get_observation()\n","        info = {} # Optional information\n","\n","        return observation, info\n","\n","    def _get_observation(self):\n","        # Get the price data window\n","        window_start = self.current_step - self.window_size\n","        window_end = self.current_step\n","        price_window = self.data['Close'].iloc[window_start:window_end].values\n","\n","        # Normalize the price window\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        # Fit on the entire historical data to maintain consistent scaling\n","        # Alternatively, fit on a rolling window if you want adaptive scaling\n","        scaled_price_window = scaler.fit_transform(price_window.reshape(-1, 1)).flatten()\n","\n","        # Get the current price for prediction\n","        current_price_sequence = self.data['Close'].iloc[self.current_step - self.window_size + 1 : self.current_step + 1].values\n","        current_price_sequence_scaled = scaler.transform(current_price_sequence.reshape(-1, 1)).flatten()\n","        # Reshape for the LSTM model input\n","        current_batch = current_price_sequence_scaled.reshape(1, self.window_size, 1)\n","\n","        # Get the price prediction\n","        # Ensure the LSTM model is loaded and available\n","        # Assuming 'model' is your trained LSTM prediction model\n","        try:\n","            next_price_prediction_scaled = model.predict(current_batch)\n","            next_price_prediction = scaler.inverse_transform(next_price_prediction_scaled)[0, 0]\n","            # Scale the predicted price to fit the observation space range (0, 1)\n","            # This requires fitting the scaler on the entire dataset or a consistent range\n","            # A simple approach is to scale based on the min/max of the data the RL agent sees\n","            # For simplicity, let's scale relative to the current window's price range\n","            # You'll need a more robust scaling strategy in a real application\n","            min_price_window = price_window.min()\n","            max_price_window = price_window.max()\n","            predicted_price_scaled_obs = (next_price_prediction - min_price_window) / (max_price_window - min_price_window)\n","            predicted_price_scaled_obs = np.clip(predicted_price_scaled_obs, 0, 1) # Ensure it's within [0, 1]\n","        except NameError:\n","             # Handle the case where the LSTM model is not defined\n","             # You should ensure 'model' is loaded before creating the environment\n","             print(\"LSTM prediction model 'model' not found. Ensure it's loaded before initializing the environment.\")\n","             # Return a default or placeholder value for the prediction\n","             predicted_price_scaled_obs = 0.5 # Or some other reasonable default\n","\n","        # Get the pivot points for the current window\n","        # This is a placeholder. You need to integrate your pivot point calculation logic here.\n","        # Based on the Birch clustering, you have `center_prices` (from MinMaxScaler)\n","        # and `cluster_supports`, `cluster_resistances` (from OHLCV clustering).\n","        # You need to determine which pivot points are relevant for the current step.\n","        # A simple approach is to find the nearest support and resistance levels from the cluster centers.\n","        try:\n","             # Find the nearest support and resistance from the calculated centers\n","             current_price = self.data['Close'].iloc[self.current_step]\n","             if 'center_prices' in globals() and len(center_prices) > 0:\n","                  nearest_support = np.max(center_prices[center_prices <= current_price]) if np.any(center_prices <= current_price) else np.min(center_prices)\n","                  nearest_resistance = np.min(center_prices[center_prices >= current_price]) if np.any(center_prices >= current_price) else np.max(center_prices)\n","\n","                  # Scale the distance to nearest support/resistance relative to the current price\n","                  distance_to_support = (current_price - nearest_support) / current_price\n","                  distance_to_resistance = (nearest_resistance - current_price) / current_price\n","\n","                  # Scale these distances to fit within the observation space range (e.g., using min/max from the entire dataset)\n","                  # For simplicity here, let's just include the raw distances for now.\n","                  # You MUST implement proper scaling for these features.\n","                  # Let's scale them based on a hypothetical maximum relative distance (e.g., 100%)\n","                  scaled_distance_to_support = np.clip(distance_to_support, -1, 1) # Assuming distance can be positive or negative if price is below support\n","                  scaled_distance_to_resistance = np.clip(distance_to_resistance, -1, 1)\n","             else:\n","                  scaled_distance_to_support = 0.5 # Default if no pivot points are available\n","                  scaled_distance_to_resistance = 0.5\n","\n","        except NameError:\n","             print(\"Pivot points (center_prices) not found. Ensure Birch clustering was run.\")\n","             scaled_distance_to_support = 0.5\n","             scaled_distance_to_resistance = 0.5\n","\n","\n","        # Combine all features into the observation\n","        # Ensure all features are in the range [0, 1] if using Box(low=0, high=1)\n","        observation = np.concatenate((\n","            scaled_price_window,\n","            [predicted_price_scaled_obs],\n","            [scaled_distance_to_support, scaled_distance_to_resistance]\n","        ))\n","\n","        # Ensure the observation has the correct shape\n","        assert observation.shape == self.observation_space.shape, f\"Observation shape mismatch: {observation.shape} vs {self.observation_space.shape}\"\n","\n","        return observation\n","\n","    def step(self, action):\n","        self.current_step += 1\n","        terminated = self.current_step >= self.max_steps\n","        reward = 0\n","        info = {}\n","\n","        current_price = self.data['Close'].iloc[self.current_step]\n","\n","        if action == 0: # Sell\n","            if self.shares_held > 0:\n","                sell_price = current_price\n","                self.balance += self.shares_held * sell_price\n","                self.shares_held = 0\n","                # Calculate reward based on profit/loss from selling\n","                # This is a simplified reward. A real trading environment needs a sophisticated reward function.\n","                # For example, reward could be based on the percentage return of the trade.\n","                trade_profit_loss = (sell_price - self.positions[-1]['buy_price']) * self.positions[-1]['shares'] if self.positions else 0\n","                reward += trade_profit_loss # Simple reward: profit/loss of the last trade\n","                self.positions = [] # Clear positions after selling\n","        elif action == 2: # Buy\n","            if self.balance > current_price: # Check if we have enough balance to buy at least one share\n","                buy_price = current_price\n","                # Decide how many shares to buy - a simple approach is to buy a fixed amount or use a percentage of balance\n","                shares_to_buy = self.balance // buy_price # Buy as many whole shares as possible\n","                if shares_to_buy > 0:\n","                  self.shares_held += shares_to_buy\n","                  self.balance -= shares_to_buy * buy_price\n","                  self.positions.append({'buy_price': buy_price, 'shares': shares_to_buy})\n","                  # Simple reward: a small positive reward for taking a buy action\n","                  # reward += 0.01 # Encourage buying\n","\n","        # Calculate net worth at the end of the step\n","        self.net_worth = self.balance + self.shares_held * current_price\n","\n","        # Additional reward considerations:\n","        # - Reward for increasing net worth\n","        # - Penalize holding for too long without profitable trades\n","        # - Penalize selling at a loss\n","        # - Reward for making a profitable trade\n","        # - Incorporate transaction costs\n","\n","        # Simple reward based on the change in net worth\n","        # This can be problematic as it encourages holding assets even if price drops\n","        # Let's refine this: Reward the change in net worth only when a position is closed (sold)\n","        # If you want a reward at every step, it needs to be more nuanced.\n","\n","        # Example of a more nuanced reward (simplified):\n","        # Reward proportional to the percentage change in net worth during the step.\n","        # This encourages actions that lead to net worth increase.\n","        # Ensure you handle the case where net_worth_before is zero if starting with 0 balance.\n","        # Let's calculate episode return for a final reward instead.\n","        # reward = (self.net_worth - self.initial_net_worth) # This will give cumulative profit/loss as reward\n","\n","        # Let's use episode return and provide a final reward\n","        self.episode_return = self.net_worth - self.initial_net_worth # assuming initial_net_worth is set in reset\n","\n","        if terminated:\n","            # Provide the final reward at the end of the episode\n","            # The reward is the total profit/loss of the episode\n","            reward = self.episode_return\n","\n","        observation = self._get_observation()\n","\n","        # Stub for truncated flag (if episode ends early due to conditions other than termination)\n","        truncated = False\n","\n","        return observation, reward, terminated, truncated, info\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TblS6NEeFAg4"},"outputs":[],"source":["# le and has a 'Close' column and a DateTime index.\n","\n","# 1. Imports (already done at the top, just ensuring they are present for clarity)\n","# import gymnasium as gym\n","# from stable_baselines3 import PPO\n","# from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n","# from sb3_contrib import RecurrentPPO\n","# from stable_baselines3.common.envs import DummyVecEnv\n","# from stable_baselines3.common.vec_env import VecNormalize\n","# from enum import IntEnum\n","# import numpy as np\n","# import pandas as pd\n","# import torch as th\n","# from torch import nn\n","\n","# 2. Define the Action Enum and TradingEnv (already provided)\n","# Make sure the TradingEnv is correctly implemented and the observation space matches the features used.\n","# The current `TradingEnv` definition uses a Box observation space of shape `(self.window_size + 1 + 2,)`.\n","\n","# Correct the `TradingEnv` to use the defined `Action` IntEnum\n","class TradingEnv(gym.Env):\n","    def __init__(self, data, window_size=60):\n","        super(TradingEnv, self).__init__()\n","\n","        self.data = data\n","        self.window_size = window_size\n","        self.current_step = self.window_size # Start after the initial window\n","        self.max_steps = len(self.data) - 1\n","\n","        # Action space: Use the defined Action Enum\n","        self.action_space = gym.spaces.Discrete(len(Action))\n","\n","        # Observation space: We'll include the price window, the price prediction,\n","        # and the current price's relation to recent pivot points.\n","        # The size will depend on the window size + prediction features + pivot features.\n","        # Let's assume we add 1 for the price prediction and 2 for pivot relation (e.g., distance to nearest support/resistance).\n","        # This is a simplified example, you'll need to carefully design your observation space.\n","        self.observation_space = gym.spaces.Box(\n","            low=-np.inf, high=np.inf, shape=(self.window_size + 1 + 2,), dtype=np.float32\n","            # Using -np.inf and np.inf as we might not have strict bounds on scaled features\n","            # If you ensure all features are scaled to [0, 1], you can keep low=0, high=1\n","        )\n","\n","        # Initial state and other attributes\n","        self.initial_balance = 10000 # Define this as an attribute\n","        self.reset() # Call reset here to initialize attributes like balance, shares_held, etc.\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.current_step = self.window_size\n","        self.balance = self.initial_balance # Starting balance\n","        self.shares_held = 0\n","        self.net_worth = self.balance\n","        self.positions = [] # Track buy/sell actions and prices\n","        # self.episode_return = 0 # This will be calculated based on final net_worth - initial_net_worth\n","        self.initial_net_worth = self.net_worth # Store initial net worth\n","\n","        # Get the initial observation\n","        observation = self._get_observation()\n","        info = {} # Optional information\n","\n","        return observation, info\n","\n","    def _get_observation(self):\n","        # Get the price data window\n","        window_start = self.current_step - self.window_size\n","        window_end = self.current_step\n","        price_window = self.data['Close'].iloc[window_start:window_end].values\n","\n","        # Normalize the price window using MinMaxScaler fit on the *entire* dataset\n","        # Or fit on a sufficiently large historical window to avoid lookahead bias if simulating trading\n","        # For training, fitting on a representative historical period might be acceptable.\n","        # Let's assume a global scaler or fit on the data used for training the RL agent.\n","        # A simple approach for this example is to fit on the current window, but be aware of bias.\n","        # A more robust approach uses a scaler fit on a large historical dataset or a rolling fit.\n","\n","        # For demonstration, let's create a scaler instance for the current window prices\n","        # In a real scenario, use a pre-fit scaler or a rolling scaler.\n","        temp_scaler_price_window = MinMaxScaler(feature_range=(0, 1))\n","        # Need to reshape for the scaler\n","        scaled_price_window = temp_scaler_price_window.fit_transform(price_window.reshape(-1, 1)).flatten()\n","\n","\n","        # Get the current price sequence for prediction\n","        # Ensure the sequence has the correct length (window_size) for the Keras model\n","        prediction_sequence_start = self.current_step - self.window_size\n","        prediction_sequence_end = self.current_step\n","        current_price_sequence = self.data['Close'].iloc[prediction_sequence_start : prediction_sequence_end].values\n","\n","        # Scale this sequence using the same scaler used for the Keras model training\n","        # Assuming 'scaler' from the Keras preprocessing section is available globally or passed.\n","        # This 'scaler' was fit on the 'Close' price data used for Keras training.\n","        try:\n","            current_price_sequence_scaled = scaler.transform(current_price_sequence.reshape(-1, 1)).flatten()\n","            # Reshape for the LSTM model input: (1, window_size, 1)\n","            current_batch = current_price_sequence_scaled.reshape(1, self.window_size, 1)\n","\n","            # Get the price prediction using the trained Keras model\n","            next_price_prediction_scaled = model.predict(current_batch, verbose=0) # Add verbose=0 to reduce output\n","            next_price_prediction = scaler.inverse_transform(next_price_prediction_scaled)[0, 0]\n","\n","            # Scale the predicted price to fit the observation space range (e.g., [0, 1])\n","            # We need a consistent way to scale this predicted price.\n","            # One approach is to scale it relative to the min/max of the entire training data the RL agent sees.\n","            # Or scale it relative to the range of the current price window (less robust).\n","            # Let's re-use the temp_scaler_price_window from the current window for simplicity in this example.\n","            # A better approach is to scale based on a long-term price range.\n","            predicted_price_scaled_obs = temp_scaler_price_window.transform(np.array([[next_price_prediction]]))[0, 0]\n","            predicted_price_scaled_obs = np.clip(predicted_price_scaled_obs, -1, 2) # Clip to a reasonable range, not necessarily [0, 1] if using -inf, inf\n","\n","        except NameError:\n","             print(\"Keras prediction model 'model' or scaler not found. Ensure they are available.\")\n","             # Return a default or placeholder value for the prediction\n","             predicted_price_scaled_obs = 0.0 # Using 0.0 as a default placeholder\n","\n","\n","        # Get the pivot points relation for the current step\n","        # Assuming `center_prices` (from Birch clustering) is available globally or passed.\n","        try:\n","             current_price = self.data['Close'].iloc[self.current_step]\n","             scaled_distance_to_support = 0.0 # Placeholder\n","             scaled_distance_to_resistance = 0.0 # Placeholder\n","\n","             if 'center_prices' in globals() and len(center_prices) > 0:\n","                  # Find the nearest support and resistance from the calculated centers\n","                  supports = center_prices[center_prices <= current_price]\n","                  resistances = center_prices[center_prices >= current_price]\n","\n","                  nearest_support = np.max(supports) if len(supports) > 0 else np.min(center_prices) # Use closest if none below, or min of all\n","                  nearest_resistance = np.min(resistances) if len(resistances) > 0 else np.max(center_prices) # Use closest if none above, or max of all\n","\n","                  # Calculate distance to nearest support/resistance\n","                  distance_to_support = current_price - nearest_support\n","                  distance_to_resistance = nearest_resistance - current_price\n","\n","                  # Scale these distances. A simple approach is to divide by current price or a moving average.\n","                  # Or, scale relative to the price range of the trading data.\n","                  # For this example, let's just use the raw distances for now, but you should scale them.\n","                  # If using raw distances, the observation space low/high should be adjusted.\n","                  # Let's try scaling by the current price for a relative distance.\n","                  # Add a small epsilon to avoid division by zero if current_price is 0.\n","                  epsilon = 1e-8\n","                  scaled_distance_to_support = distance_to_support / (current_price + epsilon)\n","                  scaled_distance_to_resistance = distance_to_resistance / (current_price + epsilon)\n","\n","\n","             else:\n","                  scaled_distance_to_support = 0.0 # Default if no pivot points\n","                  scaled_distance_to_resistance = 0.0\n","\n","        except NameError:\n","             print(\"Pivot points (center_prices) not found. Ensure Birch clustering was run.\")\n","             scaled_distance_to_support = 0.0\n","             scaled_distance_to_resistance = 0.0\n","\n","\n","        # Combine all features into the observation\n","        observation = np.concatenate((\n","            scaled_price_window,\n","            [predicted_price_scaled_obs],\n","            [scaled_distance_to_support, scaled_distance_to_resistance]\n","        ))\n","\n","        # Ensure the observation has the correct shape\n","        assert observation.shape == self.observation_space.shape, f\"Observation shape mismatch: {observation.shape} vs {self.observation_space.shape}\"\n","\n","        return observation\n","\n","    def step(self, action):\n","        # Action mapping based on the Action Enum\n","        action = Action(action)\n","\n","        self.current_step += 1\n","        terminated = self.current_step >= self.max_steps\n","        reward = 0\n","        info = {}\n","\n","        current_price = self.data['Close'].iloc[self.current_step]\n","\n","        # Implement trading logic based on the defined actions\n","        if action == Action.LONG_ENTER:\n","            # Check if we can enter a long position (e.g., not already in a position, enough balance)\n","            if self.shares_held == 0 and self.balance > current_price:\n","                shares_to_buy = self.balance // current_price\n","                if shares_to_buy > 0:\n","                    self.shares_held += shares_to_buy\n","                    self.balance -= shares_to_buy * current_price\n","                    # Record the entry price and shares for this position\n","                    self.positions.append({'type': 'long', 'entry_price': current_price, 'shares': shares_to_buy})\n","                    # Optional: Small positive reward for entering a position (depends on strategy)\n","                    # reward += 0.01\n","\n","        elif action == Action.LONG_EXIT:\n","            # Check if we are in a long position to exit\n","            if self.shares_held > 0 and len(self.positions) > 0 and self.positions[-1]['type'] == 'long':\n","                exit_price = current_price\n","                entry_price = self.positions[-1]['entry_price']\n","                shares = self.positions[-1]['shares']\n","\n","                self.balance += self.shares_held * exit_price\n","                self.shares_held = 0\n","                self.positions = [] # Clear position\n","\n","                # Calculate reward for this trade\n","                profit_loss = (exit_price - entry_price) * shares\n","                reward += profit_loss # Reward is the profit/loss of the trade\n","\n","        elif action == Action.SHORT_ENTER:\n","             # Implement short entry logic (more complex: involves borrowing shares)\n","             # For simplicity, let's not implement shorting in this example, or make it a no-op\n","             pass # No shorting for now\n","\n","        elif action == Action.SHORT_EXIT:\n","             # Implement short exit logic\n","             pass # No shorting for now\n","\n","        elif action == Action.NEUTRAL:\n","            # Holding position or staying out of the market\n","            # No transaction, potentially a small holding penalty or reward based on market movement\n","            pass\n","\n","        # Calculate net worth at the end of the step\n","        self.net_worth = self.balance + self.shares_held * current_price\n","\n","        # Additional rewards/penalties can be added here, e.g.,\n","        # - Time penalty for holding a losing position\n","        # - Reward for increasing net worth over time (but be careful with this)\n","        # - Penalties for illegal actions (e.g., trying to sell when no shares are held)\n","\n","        # Final reward is given at the end of the episode\n","        if terminated:\n","            # Calculate the total profit/loss for the episode\n","            total_episode_return = self.net_worth - self.initial_net_worth\n","            reward += total_episode_return # Reward the total profit/loss\n","\n","        observation = self._get_observation()\n","\n","        # Stub for truncated flag (if episode ends early due to conditions other than termination)\n","        truncated = False\n","\n","        info = {\"net_worth\": self.net_worth} # Add info if needed\n","\n","        return observation, reward, terminated, truncated, info\n","\n","    def render(self):\n","        # Implement rendering if needed (e.g., plotting the trading process)\n","        pass\n","\n","    def close(self):\n","        # Clean up resources if needed\n","        pass\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F0hsYx6jD5kE"},"source":["## Define Agents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9mlgtljEVa1"},"outputs":[],"source":["\n","import torch as th\n","from torch import nn\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n","import gymnasium as gym\n","from gymnasium import spaces\n","from enum import IntEnum\n","from datetime import timedelta\n","\n","# Custom Feature Extractor\n","class CustomFeatureExtractor(BaseFeaturesExtractor):\n","    \"\"\"\n","    :param observation_space: (gym.Space)\n","    :param features_dim: (int) Number of features extracted.\n","        This corresponds to the number of unit for the last layer.\n","    \"\"\"\n","    def __init__(self, observation_space: gym.Space, features_dim: int = 64):\n","        super().__init__(observation_space, features_dim)\n","        # Assuming observation space is Box((window_size + 1 + 2),)\n","        n_features = observation_space.shape[0]\n","        self.linear = nn.Sequential(nn.Linear(n_features, features_dim), nn.ReLU())\n","\n","    def forward(self, observations: th.Tensor) -> th.Tensor:\n","        return self.linear(observations)\n","\n","# Define the custom policy network\n","class CustomPolicy(nn.Module):\n","    def __init__(self, feature_extractor, lstm_hidden_size, action_space):\n","        super(CustomPolicy, self).__init__()\n","        self.feature_extractor = feature_extractor\n","        extracted_features_dim = feature_extractor.features_dim\n","\n","        # LSTM layer\n","        # Assuming the feature extractor outputs a flat vector per timestep\n","        # We need to reshape the observation to have a time dimension for LSTM\n","        # For simplicity, let's assume the observation is (batch_size, feature_dim)\n","        # and we treat each step as a single timestep for LSTM.\n","        # If your observation includes sequences (like the price window), you need to\n","        # process the sequence part separately or design the feature extractor\n","        # to handle sequences.\n","        # For now, let's assume the feature extractor flattens the observation.\n","        # If your observation is (batch_size, seq_len, features), adjust LSTM input_size and reshape.\n","        self.lstm = nn.LSTM(extracted_features_dim, lstm_hidden_size, batch_first=True)\n","\n","        # Policy and value heads\n","        self.policy_net = nn.Sequential(\n","            nn.Linear(lstm_hidden_size, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, action_space.n)\n","        )\n","        self.value_net = nn.Sequential(\n","            nn.Linear(lstm_hidden_size, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, observations: th.Tensor, hidden_state):\n","        # Reshape observations for LSTM: (batch_size, sequence_length, input_size)\n","        # Here, sequence_length is 1 as we process one observation at a time from the feature extractor\n","        # If your feature extractor returns a sequence, adjust accordingly.\n","        features = self.feature_extractor(observations) # shape: (batch_size, extracted_features_dim)\n","        features = features.unsqueeze(1) # shape: (batch_size, 1, extracted_features_dim)\n","\n","        lstm_out, new_hidden_state = self.lstm(features, hidden_state) # shape: (batch_size, 1, lstm_hidden_size)\n","        lstm_out = lstm_out.squeeze(1) # shape: (batch_size, lstm_hidden_size)\n","\n","        action_probs = self.policy_net(lstm_out)\n","        value = self.value_net(lstm_out)\n","\n","        return action_probs, value, new_hidden_state\n","\n","    def _get_action_dist_from_latent(self, latent_pi: th.Tensor):\n","        \"\"\"\n","        Retrieve action distribution from the latent representation of the policy.\n","\n","        :param latent_pi: (th.Tensor) Latent representation of the policy.\n","        :return: (CategoricalDistribution) Action distribution.\n","        \"\"\"\n","        # This is a placeholder. You'll need to implement this based on your policy head structure.\n","        # For a discrete action space and a final layer outputting logits:\n","        from stable_baselines3.common.distributions import CategoricalDistribution\n","        logits = self.policy_net(latent_pi)\n","        return CategoricalDistribution(logits=logits)\n","\n","    def evaluate_actions(self, observations: th.Tensor, actions: th.Tensor, hidden_state):\n","        \"\"\"\n","        Evaluate actions given observations.\n","\n","        :param observations: (th.Tensor) Observations.\n","        :param actions: (th.Tensor) Actions.\n","        :param hidden_state: (tuple) LSTM hidden state (h, c).\n","        :return: (th.Tensor, th.Tensor, th.Tensor) Log likelihood of actions, value estimates, entropy of distribution.\n","        \"\"\"\n","        # Reshape observations for LSTM: (batch_size, sequence_length, input_size)\n","        features = self.feature_extractor(observations) # shape: (batch_size, extracted_features_dim)\n","        features = features.unsqueeze(1) # shape: (batch_size, 1, extracted_features_dim)\n","\n","\n","        # Need to handle the sequence length and hidden state management for the PPO update.\n","        # This is complex and often handled by the Recurrent PPO implementation itself.\n","        # The Recurrent PPO in sb3-contrib expects a model that can manage the hidden state\n","        # across sequences.\n","\n","        # Let's simplify and assume the input observations already have a sequence dimension\n","        # and the hidden state is handled by the wrapper.\n","        # If using Recurrent PPO, the `forward` method is often structured differently\n","        # to process sequences and manage the hidden state.\n","\n","        # Assuming the input `observations` to `evaluate_actions` is already shaped\n","        # (batch_size, sequence_length, observation_space_shape)\n","        # We need to apply the feature extractor and LSTM timestep by timestep or as a sequence.\n","\n","        # This requires a more detailed integration with the Recurrent PPO structure.\n","        # For a standard Recurrent PPO setup, your model's `forward` method\n","        # would typically take `observations` (batch_size, sequence_length, ...)\n","        # and `lstm_states` (tuple of (batch_size, lstm_hidden_size)) and return\n","        # `actions`, `values`, `log_prob`, `lstm_states`.\n","\n","        # Let's provide a basic structure assuming the input `observations` is (batch_size, observation_shape)\n","        # and we need to process them sequentially with a dummy sequence length for demonstration.\n","        # In a real Recurrent PPO, the sequence length is determined by the sampler.\n","\n","        # Dummy sequence length for evaluation (e.g., 1 for independent steps in evaluation)\n","        # In training with RecurrentPPO, this would be the actual sequence length.\n","        sequence_length = 1\n","        batch_size = observations.shape[0]\n","\n","        # Reshape observations to add sequence length dimension: (batch_size, sequence_length, observation_shape)\n","        observations = observations.unsqueeze(1)\n","\n","\n","        # Need to pass hidden_state through the LSTM\n","        features = self.feature_extractor(observations.view(batch_size * sequence_length, -1))\n","        features = features.view(batch_size, sequence_length, -1)\n","\n","\n","        lstm_out, _ = self.lstm(features, hidden_state) # shape: (batch_size, sequence_length, lstm_hidden_size)\n","        # Take the output of the last timestep in the sequence\n","        lstm_out_last = lstm_out[:, -1, :] # shape: (batch_size, lstm_hidden_size)\n","\n","\n","        action_logits = self.policy_net(lstm_out_last)\n","        values = self.value_net(lstm_out_last)\n","\n","        # Calculate action distribution\n","        from stable_baselines3.common.distributions import CategoricalDistribution\n","        distribution = CategoricalDistribution(logits=action_logits)\n","\n","        log_prob = distribution.log_prob(actions)\n","        entropy = distribution.entropy()\n","\n","        return log_prob, values, entropy\n","\n","\n","# You would integrate this custom policy with RecurrentPPO like this:\n","# from sb3_contrib import RecurrentPPO\n","# from stable_baselines3.common.envs import DummyVecEnv\n","# from stable_baselines3.common.vec_env import VecNormalize\n","# from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n","\n","# # Create the environment\n","# env = DummyVecEnv([lambda: TradingEnv(advpp_data)])\n","# # Optional: Normalize the observation and reward\n","# # env = VecNormalize(env, norm_obs=True, norm_reward=False)\n","\n","# # Define policy kwargs to use the custom feature extractor and network architecture\n","# policy_kwargs = dict(\n","#     features_extractor_class=CustomFeatureExtractor,\n","#     features_extractor_kwargs=dict(features_dim=64), # Feature extractor output dim\n","#     # Define the network architecture for the policy and value functions after the LSTM\n","#     # This is where you would specify the layers that take the LSTM output as input\n","#     # and output the action logits and value.\n","#     # The RecurrentPPO automatically handles the LSTM layer when you specify\n","#     # 'enable_lstm': True in the policy_kwargs.\n","#     # However, if you want a custom architecture including LSTM and other layers,\n","#     # defining a custom model class (like CustomPolicy above) and integrating it\n","#     # with RecurrentPPO's `policy_aliases` or by overriding policy classes\n","#     # is a more advanced approach.\n","\n","#     # A simpler approach with RecurrentPPO is to use its built-in LSTM support\n","#     # and define the CNN/MLP features extractor that runs *before* the LSTM.\n","#     # The observation space for the LSTM will be the output of the feature extractor.\n","\n","#     # Let's go back to the simpler RecurrentPPO structure: define the feature extractor\n","#     # that processes the potentially structured observation into a flat vector,\n","#     # and let RecurrentPPO add the LSTM on top of this flat vector.\n","\n","#     # If your observation is flat (like in TradingEnv), the CustomFeatureExtractor\n","#     # is suitable to process this flat vector. RecurrentPPO will then add an LSTM\n","#     # layer that takes the output of the CustomFeatureExtractor as input.\n","\n","#     enable_lstm=True,\n","#     lstm_hidden_size=128, # Size of the LSTM hidden state\n","#     # Specify the network architecture for the policy and value heads *after* the LSTM.\n","#     # These networks take the LSTM output as input.\n","#     net_arch=[dict(pi=[64], vf=[64])] # Example: one hidden layer of 64 units for both policy and value\n","#     # You can customize this further based on your needs.\n","#     # The LSTM output size is `lstm_hidden_size`.\n","# )\n","\n","# # Instantiate the agent\n","# model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=1, policy_kwargs=policy_kwargs)\n","\n","# # Train the agent\n","# # model.learn(total_timesteps=10000)\n","\n","# # Save the model\n","# # model.save(\"recurrent_ppo_trading\")\n","\n","# # Load the trained model\n","# # model = RecurrentPPO.load(\"recurrent_ppo_trading\")\n","\n","# # Evaluate the agent\n","# # obs = env.reset()\n","# # lstm_states = None\n","# # num_envs = 1 # Number of environments\n","# # episode_starts = np.ones((num_envs,), dtype=bool)\n","# # for _ in range(1000):\n","# #     action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n","# #     obs, reward, done, info = env.step(action)\n","# #     episode_starts = done\n","# #     if done:\n","# #         obs = env.reset()\n","# #         lstm_states = None # Reset LSTM states when episode ends\n","\n","# The provided code already has a TradingEnv and a trained Keras model (`model`) for price prediction.\n","# The task is to build a *custom policy network* to integrate the various observation features\n","# (price window, prediction, pivot points) and train an RL agent.\n","\n","# Let's define the components needed for integrating with RecurrentPPO.\n","# We will use the `TradingEnv` defined previously.\n","# The observation space of `TradingEnv` is a flat Box space.\n","\n","# The features in the observation are:\n","# 1. `scaled_price_window`: `window_size` features\n","# 2. `predicted_price_scaled_obs`: 1 feature\n","# 3. `scaled_distance_to_support`: 1 feature\n","# 4. `scaled_distance_to_resistance`: 1 feature\n","# Total observation space dimension: `window_size + 1 + 2`\n","\n","# RecurrentPPO's `MlpLstmPolicy` first passes the observation through an MLP (controlled by `net_arch`)\n","# and *then* through an LSTM. This means the input to the LSTM will be the output of the MLP.\n","\n","# If we want the LSTM to process the sequence of raw (or pre-processed) observations\n","# over time steps within an episode, the `TradingEnv` needs to return observations\n","# that represent sequences, or we need a custom `features_extractor` that restructures\n","# the batch of observations from the vector environment into sequences for the LSTM.\n","# Stable-Baselines3's `VecEnv` collects observations in batches. RecurrentPPO handles\n","# the sequence creation and state passing based on the `episode_starts` information.\n","\n","# So, the standard `MlpLstmPolicy` with `enable_lstm=True` should work, with a custom\n","# `features_extractor` if the raw observation space needs initial processing before the LSTM.\n","# In our case, the `TradingEnv` already provides a flat observation vector at each step.\n","# We can use the `CustomFeatureExtractor` to process this flat vector before it goes into the LSTM.\n","\n","# Let's define the components and set up the RecurrentPPO agent.\n","# Assuming `advpp_data` (your OHLCV DataFrame used for training the Keras model)\n","# is availab\n","# 3. Define the Custom Feature Extractor (already provided)\n","class CustomFeatureExtractor(BaseFeaturesExtractor):\n","    \"\"\"\n","    :param observation_space: (gym.Space)\n","    :param features_dim: (int) Number of features extracted.\n","        This corresponds to the number of unit for the last layer.\n","    \"\"\"\n","    def __init__(self, observation_space: gym.Space, features_dim: int = 64):\n","        super().__init__(observation_space, features_dim)\n","        n_features = observation_space.shape[0]\n","        # Use a simple linear layer followed by ReLU as the feature extractor\n","        self.linear = nn.Sequential(nn.Linear(n_features, features_dim), nn.ReLU())\n","\n","    def forward(self, observations: th.Tensor) -> th.Tensor:\n","        # The input `observations` to the feature extractor for RecurrentPPO\n","        # when `enable_lstm=True` and using MlpLstmPolicy will be\n","        # (batch_size, sequence_length, observation_space_shape)\n","        # We need to process each timestep's observation.\n","        # Reshape to (batch_size * sequence_length, observation_space_shape)\n","        batch_size, sequence_length, _ = observations.shape\n","        observations = observations.view(batch_size * sequence_length, -1)\n","        features = self.linear(observations)\n","        # Reshape back to (batch_size, sequence_length, features_dim)\n","        features = features.view(batch_size, sequence_length, -1)\n","        return features\n","\n","\n","# 4. Instantiate the environment and agent\n","# Make sure `advpp_data` is loaded and available (assuming it's the DataFrame from previous cells)\n","# Also ensure the Keras `model` and `scaler` are trained and available.\n","\n","# Create the environment\n","# Use a DummyVecEnv for simplicity with a single environment\n","env = DummyVecEnv([lambda: TradingEnv(advpp_data)])\n","\n","# Optional: Normalize the observation and reward.\n","# Normalizing observations is generally recommended for neural networks.\n","# Normalizing rewards can help stabilize training.\n","env = VecNormalize(env, norm_obs=True, norm_reward=False) # Start without reward normalization\n","\n","# Define policy kwargs for RecurrentPPO\n","policy_kwargs = dict(\n","    features_extractor_class=CustomFeatureExtractor,\n","    features_extractor_kwargs=dict(features_dim=64), # Output dimension of the feature extractor\n","    enable_lstm=True, # Enable the built-in LSTM layer in MlpLstmPolicy\n","    lstm_hidden_size=128, # Hidden size of the LSTM\n","    # Define the network architecture for the policy and value heads *after* the LSTM.\n","    # The input to these networks is the output of the LSTM (lstm_hidden_size).\n","    net_arch=[dict(pi=[64], vf=[64])] # Example: one hidden layer of 64 units for both policy and value heads\n","    # The final output layer sizes are determined by the action space (for policy) and 1 (for value).\n",")\n","\n","# Instantiate the RecurrentPPO agent\n","# Use the MlpLstmPolicy provided by sb3_contrib\n","model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, tensorboard_log=\"./trading_ppo_lstm_tensorboard/\")\n","\n","# 5. Train the agent\n","print(\"Starting training...\")\n","# Adjust total_timesteps based on your data size and training goals\n","# Training time depends on the number of steps, batch size, etc.\n","try:\n","    model.learn(total_timesteps=100000) # Train for a certain number of timesteps\n","    print(\"Training finished.\")\n","except Exception as e:\n","    print(f\"An error occurred during training: {e}\")\n","\n","# 6. Save the trained model\n","try:\n","    model.save(\"recurrent_ppo_trading_policy\")\n","    print(\"Model saved successfully.\")\n","except Exception as e:\n","    print(f\"An error occurred while saving the model: {e}\")\n","\n","\n","# 7. Evaluate the trained agent (Optional)\n","# Need a separate evaluation environment\n","# eval_env = DummyVecEnv([lambda: TradingEnv(advpp_data)]) # Use a different data split for evaluation if available\n","# eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, training=False, norm_and_reward_env=False) # Use the same normalization but in evaluation mode\n","\n","# print(\"Starting evaluation...\")\n","# obs = eval_env.reset()\n","# lstm_states = None # Reset LSTM states for evaluation\n","# num_envs = eval_env.num_envs\n","# # Episode start signals for the LSTM, from VecEnv\n","# episode_starts = np.ones((num_envs,), dtype=bool)\n","\n","# total_reward = 0\n","# n_steps = 0\n","# max_eval_steps = 1000 # Number of steps to evaluate for\n","\n","# try:\n","#     for _ in range(max_eval_steps):\n","#         action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n","#         obs, reward, done, info = eval_env.step(action)\n","#         total_reward += reward\n","#         n_steps += 1\n","#         episode_starts = done # Update episode_starts based on done flags\n","\n","#         if done:\n","#              # Log or process episode end information\n","#              for i, d in enumerate(done):\n","#                   if d:\n","#                        print(f\"Episode finished after {n_steps} steps. Return: {info[i]['net_worth'] - eval_env.get_original_obs()[i]['initial_net_worth']:.2f}\") # Access original obs if needed\n","#                        # Note: accessing info from VecEnv needs careful handling, especially with done=True\n","#                        # Use `info` returned by `eval_env.step` which is a list of dictionaries for vectorized envs.\n","#              obs = eval_env.reset()\n","#              lstm_states = None # Reset LSTM states when episode ends\n","#              episode_starts = np.ones((num_envs,), dtype=bool) # All episodes start after reset\n","#              n_steps = 0 # Reset step counter if evaluating a single episode\n","\n","#     print(f\"Evaluation finished after {max_eval_steps} steps. Average reward: {total_reward / max_eval_steps:.2f}\")\n","\n","# except Exception as e:\n","#      print(f\"An error occurred during evaluation: {e}\")\n","\n","# To run this code, ensure:\n","# 1. The `advpp_data` DataFrame is loaded and contains a 'Close' column with a DateTime index.\n","# 2. The Keras `model` (for price prediction) is trained and available globally or passed to the environment.\n","# 3. The Keras `scaler` (used for scaling the price data for Keras model) is available globally or passed.\n","# 4. The `center_prices` list (from Birch clustering) is available globally or passed to the environment.\n","# 5. Install `sb3_contrib`: `!pip install sb3-contrib`\n","# 6. Install `gymnasium`: `!pip install gymnasium`\n","```"]},{"cell_type":"markdown","metadata":{"id":"XUcIsqeHBIIo"},"source":["## Train Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZvG5FJq9pJS"},"outputs":[],"source":["# --- Data Preparation ---\n","# Assuming 'ohlcv' DataFrame is available from previous steps and contains 'timestamp', 'Close'\n","# Ensure 'timestamp' is a datetime index\n","if not isinstance(ohlcv_copy.index, pd.DatetimeIndex):\n","    ohlcv_copy['timestamp'] = pd.to_datetime(ohlcv_copy['timestamp'])\n","    ohlcv_copy = ohlcv_copy.set_index('timestamp')\n","\n","# Select a subset of the data for training the RL agent\n","# Make sure this slice is long enough for your window size and training\n","rl_data = ohlcv_copy.loc['2021-01-01':'2023-01-01'].copy() # Adjust dates as needed\n","\n","# Ensure there's enough data after slicing\n","if len(rl_data) < 100: # Arbitrary minimum length\n","    raise ValueError(\"Not enough data for the specified date range. Please adjust dates.\")\n","\n","# --- Environment Setup ---\n","WINDOW_SIZE = 60 # Needs to match the window size used for price prediction\n","# Make sure the LSTM model 'model' is trained and available in the global scope\n","# If not, you need to load it here or pass it to the TradingEnv\n","\n","# Create the environment\n","env = TradingEnv(data=rl_data, window_size=WINDOW_SIZE)\n","\n","# Vectorize the environment\n","vec_env = make_vec_env(lambda: env, n_envs=1)\n","\n","# --- Model Training ---\n","# Use RecurrentPPO from sb3_contrib for LSTM policy\n","# policy='LstmPolicy' uses a built-in LSTM network.\n","# You might need a custom policy network to integrate the various observation features.\n","# For a simple start, let's use the default LstmPolicy and ensure the observation space is flat.\n","\n","# The observation space is already flattened in _get_observation.\n","\n","# Initialize the agent\n","# Adjust hyperparameters as needed\n","model_rl = RecurrentPPO(\"MlpLstmPolicy\", vec_env, verbose=1, device=\"auto\")\n","\n","# Train the agent\n","# Adjust total_timesteps based on your data size and desired training length\n","model_rl.learn(total_timesteps=10000) # Increase for better training\n","\n","# --- Save the trained agent ---\n","model_rl.save(\"recurrent_ppo_trading_agent\")\n"]},{"cell_type":"markdown","metadata":{"id":"h0vbaEd4A_lr"},"source":["## Evaluate Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBflJYNL_iA-"},"outputs":[],"source":["\n","# --- Evaluation (Optional) ---\n","# Load the trained agent\n","# model_rl = RecurrentPPO.load(\"recurrent_ppo_trading_agent\")\n","\n","# Run the trained agent on a test set\n","# test_data = ohlcv.loc['2023-01-02':'2024-01-01'].copy() # Adjust dates\n","# test_env = TradingEnv(data=test_data, window_size=WINDOW_SIZE)\n","# obs, _ = test_env.reset()\n","# terminated, truncated = False\n","# total_reward = 0\n","# while not terminated and not truncated:\n","#     action, _states = model_rl.predict(obs, deterministic=True)\n","#     obs, reward, terminated, truncated, info = test_env.step(action)\n","#     total_reward += reward\n","#     # Render the environment if it supports it (optional)\n","#     # test_env.render()\n","\n","# print(f\"Total reward on test set: {total_reward}\")\n","\n","# --- Integration with Pivots and Predictions ---\n","# The current environment already incorporates the price prediction and pivot point information\n","# (as implemented in the _get_observation method).\n","# The LSTM policy learns to use these features to make decisions (Buy/Hold/Sell).\n","\n","# Further steps for a more robust integration:\n","# 1. Refine the observation space: Experiment with different ways to represent pivot points\n","#    (e.g., distance to nearest support/resistance, number of pivots in a window,\n","#    whether the current price is above/below a pivot zone).\n","#    Include more prediction horizons (e.g., 1-day, 4-day, 7-day predictions).\n","#    Include other relevant features (e.g., volume, technical indicators, sentiment).\n","# 2. Design a sophisticated reward function: This is crucial for training an effective trading agent.\n","#    Consider factors like transaction costs, risk, drawdown, Sharpe ratio, etc.\n","# 3. Hyperparameter tuning: Optimize the RL agent's hyperparameters for better performance.\n","# 4. Custom Policy Network: Implement a custom policy network in stable-baselines3\n","#    to have more control over how the LSTM processes the various observation features.\n","#    For example, you might want separate branches for price window processing,\n","#    prediction features, and pivot features before combining them.\n","# 5. Backtesting: Rigorously backtest the trained agent on unseen data to evaluate its profitability and risk.\n","# 6. Training Data: Ensure you have sufficient and representative data for training.\n","#    Consider using data from multiple instruments or timeframes.\n"]},{"cell_type":"markdown","metadata":{"id":"nsjiPgeMBL8l"},"source":["## Live Decision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h89K6q_hBvjd"},"outputs":[],"source":["from gymnasium.wrappers import TimeLimit\n","from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.callbacks import BaseCallback\n","import gymnasium as gym\n","from datetime import timedelta\n","\n","# Assuming 'model' is your trained LSTM prediction model and 'scaler' is its scaler\n","\n","def make_live_decision(current_price_data: pd.DataFrame, model, scaler, pivot_centers) -> str:\n","    \"\"\"\n","    Makes a live trading decision (Buy, Hold, Sell) based on the current price data,\n","    LSTM prediction, and pivot points using a trained RL agent.\n","\n","    Args:\n","        current_price_data: DataFrame containing the latest price data,\n","                            including the window size required by the LSTM model.\n","                            Must have a 'Close' column.\n","        model: The trained Keras LSTM prediction model.\n","        scaler: The MinMaxScaler used to scale the training data for the LSTM model.\n","        pivot_centers: A list or array of pivot point prices (from Birch clustering).\n","\n","    Returns:\n","        A string indicating the recommended action: 'Buy', 'Hold', or 'Sell'.\n","    \"\"\"\n","    WINDOW_SIZE = 60 # Must match the window size used for RL training and prediction\n","\n","    if len(current_price_data) < WINDOW_SIZE:\n","        print(f\"Not enough data for the window size ({WINDOW_SIZE}). Cannot make a decision.\")\n","        return 'Hold' # Default to hold if not enough data\n","\n","    # --- Prepare Observation for the RL Agent ---\n","\n","    # Get the price data window (latest WINDOW_SIZE closing prices)\n","    price_window = current_price_data['Close'].values[-WINDOW_SIZE:]\n","\n","    # Normalize the price window using the same scaler as used for prediction\n","    # Note: In a real-world scenario, you might need to handle the scaler fitting\n","    # more robustly if the price range changes significantly over time.\n","    try:\n","      # Ensure the scaler is fitted on data that includes the range of the price_window\n","      # This might require refitting the scaler or using a scaler fitted on a wider range\n","      # For simplicity here, we assume the scaler is adequate.\n","      scaled_price_window = scaler.transform(price_window.reshape(-1, 1)).flatten()\n","    except Exception as e:\n","      print(f\"Error scaling price window: {e}\")\n","      # Handle scaling error - e.g., return 'Hold' or use a default scaled window\n","      return 'Hold'\n","\n","\n","    # Get the current price for prediction input (last WINDOW_SIZE prices)\n","    current_price_sequence = current_price_data['Close'].values[-WINDOW_SIZE:]\n","    current_price_sequence_scaled = scaler.transform(current_price_sequence.reshape(-1, 1)).flatten()\n","    current_batch = current_price_sequence_scaled.reshape(1, WINDOW_SIZE, 1)\n","\n","\n","    # Get the price prediction\n","    try:\n","        next_price_prediction_scaled = model.predict(current_batch)\n","        next_price_prediction = scaler.inverse_transform(next_price_prediction_scaled)[0, 0]\n","        # Scale the predicted price for the RL observation space (0, 1)\n","        # This scaling should be consistent with how it was done in the RL environment's _get_observation\n","        # For simplicity, scale relative to the range of the current window\n","        min_price_window = price_window.min()\n","        max_price_window = price_window.max()\n","        predicted_price_scaled_obs = (next_price_prediction - min_price_window) / (max_price_window - min_price_window)\n","        predicted_price_scaled_obs = np.clip(predicted_price_scaled_obs, 0, 1) # Ensure it's within [0, 1]\n","\n","    except Exception as e:\n","         print(f\"Error getting price prediction: {e}\")\n","         predicted_price_scaled_obs = 0.5 # Default if prediction fails\n","\n","    # Get pivot point relation for the current price\n","    try:\n","         current_price = current_price_data['Close'].iloc[-1] # Get the very last price\n","         if len(pivot_centers) > 0:\n","              # Find the nearest support and resistance from the calculated centers\n","              nearest_support = np.max(pivot_centers[pivot_centers <= current_price]) if np.any(pivot_centers <= current_price) else np.min(pivot_centers)\n","              nearest_resistance = np.min(pivot_centers[pivot_centers >= current_price]) if np.any(pivot_centers >= current_price) else np.max(pivot_centers)\n","\n","              # Scale the distance consistently with the RL environment\n","              # This scaling needs to be carefully chosen and fixed.\n","              # Example: Use the maximum possible price difference in the dataset the RL was trained on\n","              # For simplicity here, using a basic relative scaling.\n","              # You MUST replace this with a proper scaling based on your RL training data.\n","              distance_to_support = (current_price - nearest_support) / current_price if current_price != 0 else 0\n","              distance_to_resistance = (nearest_resistance - current_price) / current_price if current_price != 0 else 0\n","\n","              # Scale these distances to fit within [0, 1] or another appropriate range used in the RL observation space\n","              # Assuming [-1, 1] range scaling as in the example _get_observation\n","              scaled_distance_to_support = np.clip(distance_to_support, -1, 1)\n","              scaled_distance_to_resistance = np.clip(distance_to_resistance, -1, 1)\n","              # If your observation space is [0, 1], you'd need to map [-1, 1] to [0, 1]\n","              # e.g., (scaled_distance + 1) / 2\n","              scaled_distance_to_support = (scaled_distance_to_support + 1) / 2\n","              scaled_distance_to_resistance = (scaled_distance_to_resistance + 1) / 2\n","\n","         else:\n","              scaled_distance_to_support = 0.5 # Default if no pivot points are available\n","              scaled_distance_to_resistance = 0.5\n","\n","    except Exception as e:\n","         print(f\"Error calculating pivot relation: {e}\")\n","         scaled_distance_to_support = 0.5\n","         scaled_distance_to_resistance = 0.5\n","\n","\n","    # Combine all features into the observation\n","    # Ensure the order and scaling match the RL environment's observation space exactly\n","    observation = np.concatenate((\n","        scaled_price_window,\n","        [predicted_price_scaled_obs],\n","        [scaled_distance_to_support, scaled_distance_to_resistance]\n","    ))\n","\n","    # Reshape the observation to match the RL agent's expected input shape (batch_size, observation_shape)\n","    # Since we are making one decision at a time, batch size is 1.\n","    # If using an LstmPolicy, the input shape is (batch_size, sequence_length, features) if not VecEnv\n","    # or (num_envs, observation_space.shape) for VecEnv.\n","    # The RecurrentPPO model with MlpLstmPolicy expects (num_envs, *observation_space.shape) for observation\n","    # and maintains recurrent states.\n","    # Let's assume the model was trained with a VecEnv of n_envs=1.\n","    # The observation needs to be (1, observation_space.shape).\n","\n","    observation = observation.reshape(1, -1) # Reshape for VecEnv (num_envs=1)\n","\n","    # --- Load the Trained RL Agent ---\n","    # Ensure the RL agent model is loaded\n","    try:\n","        # Assuming the model is saved as 'recurrent_ppo_trading_agent.zip' (default sb3 format)\n","        # If you saved it with a different name or format, adjust accordingly.\n","        agent = RecurrentPPO.load(\"recurrent_ppo_trading_agent\", device=\"auto\")\n","\n","        # Get the recurrent states. For a single live prediction, we usually don't\n","        # have previous recurrent states. We can start with initial states (zeros).\n","        # This is a simplification; in a real live trading system, you'd maintain\n","        # the recurrent states between decisions.\n","        # The structure of the initial states depends on the policy network (e.g., LSTM layers).\n","        # For RecurrentPPO with MlpLstmPolicy, the states are usually (n_envs, 2, n_lstm_units)\n","        # where 2 is for (hidden_state, cell_state).\n","        # Let's get the structure from the loaded model's policy.\n","        # This might require accessing internal policy details or using a wrapper.\n","        # A simpler approach for a single prediction is often to use dummy initial states,\n","        # but this might not capture the full history effect of the LSTM.\n","\n","        # A more robust way is to make a dummy prediction to get the state shape\n","        # Or examine the model's policy.\n","\n","        # Let's assume the standard MlpLstmPolicy state shape for n_envs=1\n","        num_lstm_layers = 1 # Default for MlpLstmPolicy often has one LSTM layer\n","        lstm_units = 128 # Default LSTM units in MlpLstmPolicy, check model summary if possible\n","        # Initial recurrent states (hidden and cell states for each LSTM layer)\n","        # state_shape = (num_envs, num_lstm_layers * 2, lstm_units) # Incorrect structure\n","        # Correct structure for MlpLstmPolicy: (num_envs, 2, n_lstm_units)\n","        lstm_states = np.zeros((1, 2, lstm_units), dtype=np.float32) # Assuming 1 env, 2 states (h, c), lstm_units\n","\n","        # Get the deterministic action and the next recurrent states\n","        # The `state` argument in `predict` is for recurrent policies\n","        action, next_lstm_states = agent.predict(observation, state=lstm_states, deterministic=True)\n","\n","    except FileNotFoundError:\n","        print(\"Error: Trained RL agent model not found. Please ensure 'recurrent_ppo_trading_agent.zip' exists.\")\n","        return 'Hold' # Default to hold if the agent is not found\n","    except Exception as e:\n","        print(f\"Error loading or predicting with the RL agent: {e}\")\n","        return 'Hold' # Default to hold on agent error\n","\n","\n","    # --- Interpret the Action ---\n","    # Map the integer action from the agent to a trading decision string\n","    action_map = {0: 'Sell', 1: 'Hold', 2: 'Buy'}\n","    decision = action_map.get(action.item(), 'Hold') # .item() to get scalar from numpy array\n","\n","    return decision\n","\n","# --- Example Usage ---\n","# You need to have:\n","# 1. Your trained Keras LSTM prediction 'model'\n","# 2. The 'scaler' used for the LSTM model\n","# 3. Your 'pivot_centers' from the Birch clustering\n","\n","# --- Dummy Data and Objects for Demonstration ---\n","# In a real scenario, load your actual trained model, scaler, and pivot centers\n","# Ensure these objects are available in the global scope or passed to the function\n","\n","# Dummy model and scaler (replace with your actual trained model and scaler)\n","try:\n","    # Attempt to use the pre-existing 'model' and 'scaler' from the notebook\n","    print(\"Using existing 'model' and 'scaler'.\")\n","    if 'model' not in globals() or 'scaler' not in globals():\n","         raise NameError(\"LSTM 'model' or 'scaler' not found.\")\n","    # Check if 'model' is a Keras Model\n","    if not isinstance(model, keras.Model):\n","         raise TypeError(\"'model' is not a Keras Model.\")\n","    # Check if 'scaler' is a MinMaxScaler\n","    if not isinstance(scaler, MinMaxScaler):\n","         raise TypeError(\"'scaler' is not a MinMaxScaler.\")\n","\n","except (NameError, TypeError) as e:\n","    print(f\"Could not use existing 'model' and 'scaler': {e}\")\n","    print(\"Creating dummy model and scaler for demonstration. REPLACE THIS.\")\n","    # Create dummy model and scaler for demonstration purposes\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.layers import Input, LSTM, Dense, Reshape, Dropout, BatchNormalization\n","    from tensorflow.keras.layers import Attention, Multiply\n","    from sklearn.preprocessing import MinMaxScaler\n","    import numpy as np\n","\n","    # Dummy scaler - fit it on some dummy data that spans a reasonable price range\n","    dummy_prices = np.linspace(100, 400, 5000).reshape(-1, 1) # Simulate a price range\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaler.fit(dummy_prices)\n","\n","    # Dummy LSTM model - needs to have the same input shape and expected output as your real model\n","    # This dummy model just predicts the last price scaled\n","    # Replace this with loading your actual model!\n","    def build_dummy_model(input_shape):\n","      inputs = Input(shape=input_shape)\n","      lstm_out = LSTM(units=50, return_sequences=False)(inputs)\n","      outputs = Dense(1)(lstm_out)\n","      model = Model(inputs=inputs, outputs=outputs)\n","      model.compile(optimizer='adam', loss='mse') # Compile is needed even for a dummy model\n","      return model\n","\n","    # Assuming your model expects (60, 1) input shape\n","    dummy_input_shape = (60, 1)\n","    model = build_dummy_model(dummy_input_shape)\n","    print(\"Dummy model and scaler created.\")\n","\n","\n","# Dummy pivot centers (replace with your actual calculated pivot centers)\n","try:\n","    if 'center_prices' in globals() and len(center_prices) > 0:\n","        print(\"Using existing 'center_prices' for pivot points.\")\n","        pivot_centers = center_prices\n","    else:\n","         raise NameError(\"Pivot points 'center_prices' not found.\")\n","except NameError:\n","    print(\"Could not use existing 'center_prices'.\")\n","    print(\"Creating dummy pivot centers for demonstration. REPLACE THIS.\")\n","    # Create dummy pivot centers for demonstration\n","    pivot_centers = np.array([150, 200, 250, 300, 350]) # Example pivot levels\n","    print(\"Dummy pivot centers created.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_cNB4NVB1Ou"},"outputs":[],"source":["# Fetch the latest data required for the observation window\n","# Fetch enough data to cover the window size\n","INSTRUMENT = 'AAPL' # Use your instrument\n","WINDOW_SIZE = 60\n","data_period = f'{WINDOW_SIZE+5}d' # Fetch a bit more data than the window size\n","latest_data = yf.download(INSTRUMENT, period=data_period, interval='1d')\n","\n","if latest_data.empty:\n","    print(f\"Could not fetch latest data for {INSTRUMENT}. Cannot make a decision.\")\n","else:\n","    # Ensure the data is sorted by date\n","    latest_data.sort_index(inplace=True)\n","\n","    # Make the live decision\n","    decision = make_live_decision(latest_data, model, scaler, pivot_centers)\n","    print(f\"\\nLive Trading Decision for {INSTRUMENT}: {decision}\")\n","\n"]}]}